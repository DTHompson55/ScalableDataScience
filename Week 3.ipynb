{"nbformat_minor": 1, "cells": [{"source": "# Exercise 2\n## Part 1\nNow let's calculate covariance and correlation by ourselves using ApacheSpark\n\n1st we crate two random RDD\u2019s, which shouldn't correlate at all.\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import random\nrddX = sc.parallelize(random.sample(range(100),100))\nrddY = sc.parallelize(random.sample(range(100),100))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190311213801-0000\nKERNEL_ID = ad8b051e-eb20-41ff-80d6-cc41afd5724e\n"}], "metadata": {}}, {"source": "Now we calculate the mean, note that we explicitly cast the denominator to float in order to obtain a float instead of int", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "meanX = rddX.sum()/float(rddX.count())\nmeanY = rddY.sum()/float(rddY.count())\nprint (meanX)\nprint (meanY)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "49.5\n49.5\n"}], "metadata": {}}, {"source": "Now we calculate the covariance", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "rddXY = rddX.zip(rddY)\ncovXY = rddXY.map(lambda x_y : (x_y[0]-meanX)*(x_y[1]-meanY)).sum()/rddXY.count()\ncovXY", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "-50.96"}, "metadata": {}}], "metadata": {}}, {"source": "Covariance is not a normalized measure. Therefore we use it to calculate correlation. But before that we need to calculate the indivicual standard deviations first", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "from math import sqrt\nn = rddXY.count()\nsdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda x : pow(x-meanY,2)).sum()/n)\nprint (sdX)\nprint (sdY)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "28.86607004772212\n28.86607004772212\n"}], "metadata": {}}, {"source": "Now we calculate the correlation", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "corrXY = covXY / (sdX * sdY)\ncorrXY", "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "-0.06115811581158116"}, "metadata": {}}], "metadata": {}}, {"source": "## Part 2\nNo we want to create a correlation matrix out of the four RDDs used in the lecture", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "from pyspark.mllib.stat import Statistics\nimport random\ncolumn1 = sc.parallelize(range(100))\ncolumn2 = sc.parallelize(range(100,200))\ncolumn3 = sc.parallelize(list(reversed(range(100))))\ncolumn4 = sc.parallelize(random.sample(range(100),100))\ndata = column1.zip(column2).zip(column3).zip(column4).map(lambda a_b_c_d : (a_b_c_d[0][0][0],a_b_c_d[0][0][1],a_b_c_d[0][1],a_b_c_d[1]) ).map(lambda a_b_c_d : [a_b_c_d[0],a_b_c_d[1],a_b_c_d[2],a_b_c_d[3]])\nprint(Statistics.corr(data))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[ 1.          1.         -1.          0.06967897]\n [ 1.          1.         -1.          0.06967897]\n [-1.         -1.          1.         -0.06967897]\n [ 0.06967897  0.06967897 -0.06967897  1.        ]]\n"}], "metadata": {}}, {"source": "Congratulations, you are done with Exercice 2", "cell_type": "markdown", "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark", "name": "python3", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}